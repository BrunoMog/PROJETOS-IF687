{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK9vmH4R0k8oyWGbRQXGs9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrunoMog/PROJETOS-IF687/blob/main/mlp_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Início da implementação\n"
      ],
      "metadata": {
        "id": "DFYW04wjZtDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importanto biblioteca\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8liRcfYnTpKX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setando parâmetros da rede mlp\n",
        "\n",
        "input_size = 2\n",
        "hidden_size = 5\n",
        "output_size = 7\n",
        "\n",
        "#cada camada intermediária pode ter o mesmo número de unidades computacionais para fins de simplificação ou apenas terem valores diferentes, nessa implementação vamos adotar a segunda abordagem\n",
        "#para cada camada intermediária - hidden layer, terá um valor relacionado a quantidade de unidades computacionais\n",
        "\n",
        "hidden_layer_size = [5, 5, 5, 5, 5]\n",
        "\n",
        "# serão implementadas três funções de ativação para as camadas intermediárias a relu, sigmoid e linear pela qual será indicada, e para a camdada de saída conforme o indicado sigmoid ou linear\n",
        "# mas a relu também é uma opção\n",
        "# a primeira função será para camadas intermediárias e a segunda para a camada de saída\n",
        "\n",
        "activation_functions = ['relu', 'relu']\n",
        "\n",
        "# o número máximo de épocas que o modelo será treinado\n",
        "# o objetivo é que não precise chegar ao número máximo de épocas, quando o modelo apresentar overfitting o treinamento já acabe\n",
        "\n",
        "MAX_EPOCH = 1\n",
        "\n",
        "# taxa de aprendizado que será utilizada\n",
        "\n",
        "learning_rate = 0.03\n",
        "\n",
        "# tamanho dos mini-batchs que será utilizado dos dados de treinamento e teste\n",
        "\n",
        "batch_size_train = 100\n",
        "\n",
        "batch_size_test = 20"
      ],
      "metadata": {
        "id": "MpHczcWLTxNI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setando o gerador de números aleatórios para setar os pesos iniciais\n",
        "\n",
        "weigths_seed = 115\n",
        "rng = np.random.default_rng(weigths_seed)\n",
        "\n",
        "# como a rede mlp é totalmente conectada, o número de pesos gerados entre camadas segirá sempre o padrão layer[i]*layer[i+1]\n",
        "\n",
        "weights = []\n",
        "\n",
        "for i in range(hidden_size+1):\n",
        "\n",
        "  if i == 0:\n",
        "\n",
        "    weights.append(rng.random(size = (input_size, hidden_layer_size[i], 1)))\n",
        "\n",
        "  elif i < hidden_size:\n",
        "\n",
        "    weights.append(rng.random((hidden_layer_size[i-1], hidden_layer_size[i], 1)))\n",
        "\n",
        "  else:\n",
        "\n",
        "    weights.append(rng.random((hidden_layer_size[i-1], output_size, 1)))\n",
        "\n"
      ],
      "metadata": {
        "id": "xJc4i5gCWSGx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# primeiro index é para referir a qual camada, sempre haverá hidden_layers+1 camada de pesos\n",
        "# o segundo index para referenciar qual unidade computacional da camada\n",
        "# o terceiro index para referenciar o peso a qual unidade computacional da próxima camada\n",
        "# o quarto index para obter o valor\n",
        "\n",
        "print(len(weights[-1][-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1hNLJNRkptI",
        "outputId": "3836077b-84b3-49e6-b210-cf9ef623a46e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funções para o treinamento da rede MLP\n",
        "\n",
        "#### fase foward e backward\n"
      ],
      "metadata": {
        "id": "dvqQTwlOr02t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# funções de ativação\n",
        "\n",
        "# função relu normal\n",
        "def function_Relu(x):\n",
        "\n",
        "  return max(0, x)\n",
        "\n",
        "# função sigmoid normal\n",
        "def function_Sigmoid(x):\n",
        "\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "# função linear entre 0 e 1\n",
        "def function_Linear(x):\n",
        "\n",
        "  return max(0, min(1, x))"
      ],
      "metadata": {
        "id": "_n-jDHSrSlOC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# derivada das funções de ativação\n",
        "\n",
        "def derivative_Relu(x):\n",
        "\n",
        "  if x > 0:\n",
        "\n",
        "    return 1\n",
        "\n",
        "  else:\n",
        "\n",
        "    return 0\n",
        "\n",
        "# no caso da derivada da sigmoid, como o input vai ser o erro gerado pela aquiela unidade computacional\n",
        "# a derivada da sigmoide também pode se expressa por sigmoid(x)*(1-sigmoid(x)), e como sigmoid(x) é o input o output pode ser expresõ por x*(1-x)\n",
        "def derivative_Sigmoid(x):\n",
        "\n",
        "  return x*(1-x)\n",
        "\n",
        "def derivative_Linear(x):\n",
        "\n",
        "  if x > 0 and x < 1:\n",
        "\n",
        "    return 1\n",
        "\n",
        "  else:\n",
        "\n",
        "    return 0"
      ],
      "metadata": {
        "id": "jSGaNtPIB3hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# funções de custo\n",
        "\n",
        "def mean_square_loss(labels, predictions):\n",
        "\n",
        "  loss = 0\n",
        "\n",
        "  for l, p in zip(labels, predictions):\n",
        "      loss = loss + (l - p) ** 2\n",
        "\n",
        "  loss = loss / len(labels)\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "Offq8LMhrbZ7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# processo de foward da rede\n",
        "\n",
        "def foward(input_size, hidden_size, output_size, hidden_layer_size, activation_functions, weights, data):\n",
        "\n",
        "  resultado_final = []\n",
        "\n",
        "  #pra cada dado recebido como entrada, gera o valor da fase foward e armazena em resultado_final\n",
        "\n",
        "  for m in range(len(data)):\n",
        "\n",
        "    # o resultado de cada valor multiplicado pelo seu peso será armazenado no valor parcial\n",
        "    # e a saída de cada unidade computacional também será armazenada para uso posterior no backpropagation\n",
        "    output = []\n",
        "\n",
        "    resultado_parcial = [[[] for j in range(hidden_layer_size[i])] for i in range(hidden_size)]\n",
        "\n",
        "    resultado_parcial.append([[]for i in range(output_size)])\n",
        "\n",
        "    # calcular o resultado camada por camada\n",
        "\n",
        "    for i in range(hidden_size+1):\n",
        "\n",
        "      # no primeiro caso será a camada de dados\n",
        "\n",
        "      if i == 0:\n",
        "\n",
        "        input = data[j]\n",
        "\n",
        "      # o caso em que estamos calculando o resuldado parcial das camadas intermediárias\n",
        "\n",
        "      if(i < hidden_size):\n",
        "\n",
        "        for j in range(len(input)):\n",
        "\n",
        "          for k in range(hidden_layer_size[i]):\n",
        "\n",
        "            resultado_parcial[i][k].append(weights[i][j][k][0]*input[j])\n",
        "\n",
        "        # aux vai receber o resultado após a função de ativação\n",
        "\n",
        "        aux = np.zeros(hidden_layer_size[i])\n",
        "\n",
        "        # após ter os resultados parciais da soma ponderada das entradas daquela camada, esse valor irá passar pela função de ativação\n",
        "\n",
        "        if activation_functions[0] == 'relu':\n",
        "\n",
        "          for j in range(hidden_layer_size[i]):\n",
        "\n",
        "            aux[j] = function_Relu(np.sum(resultado_parcial[i][j]))\n",
        "\n",
        "        elif activation_functions[0] == 'sigmoid':\n",
        "\n",
        "          for j in range(hidden_layer_size[i]):\n",
        "\n",
        "            aux[j] = function_Sigmoid(np.sum(resultado_parcial[i][j]))\n",
        "\n",
        "        elif activation_functions[0] == 'linear':\n",
        "\n",
        "          for j in range(hidden_layer_size[i]):\n",
        "\n",
        "            aux[j] = function_Linear(np.sum(resultado_parcial[i][j]))\n",
        "\n",
        "\n",
        "      # para o caso contrário\n",
        "\n",
        "      else:\n",
        "\n",
        "        for j in range(len(input)):\n",
        "\n",
        "          for k in range(output_size):\n",
        "\n",
        "            resultado_parcial[i][k].append(weights[i][j][k][0]*input[j])\n",
        "\n",
        "        aux = np.zeros(output_size)\n",
        "\n",
        "        # após ter os resultados parciais da soma ponderada das entradas daquela camada, esse valor irá passar pela função de ativação\n",
        "\n",
        "        if activation_functions[0] == 'relu':\n",
        "\n",
        "          for j in range(output_size):\n",
        "\n",
        "            aux[j] = function_Relu(np.sum(resultado_parcial[i][j]))\n",
        "\n",
        "        elif activation_functions[0] == 'sigmoid':\n",
        "\n",
        "          for j in range(output_size):\n",
        "\n",
        "            aux[j] = function_Sigmoid(np.sum(resultado_parcial[i][j]))\n",
        "\n",
        "        elif activation_functions[0] == 'linear':\n",
        "\n",
        "          for j in range(output_size):\n",
        "\n",
        "            aux[j] = function_Linear(np.sum(resultado_parcial[i][j]))\n",
        "\n",
        "      # após ter o resultado parcial da camada, o input da próxima iteração será o resultado dessa\n",
        "\n",
        "      input = aux\n",
        "\n",
        "\n",
        "      # output vai ter os outputs de cada camada da rede\n",
        "      # o primeiro index é para a camada, apenas a primeira camada intermediária e posteriores\n",
        "      # o segundo index é para a unidade computacional\n",
        "      # o terceiro index é sempre 0 e refere ao valor\n",
        "      output.append(input)\n",
        "\n",
        "    resultado_final.append(output)\n",
        "\n",
        "  return resultado_final\n"
      ],
      "metadata": {
        "id": "OSa5J3HDGoT7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fase backward\n",
        "\n",
        "def backward(input_size, hidden_size, output_size, hidden_layer_size, activation_functions, weights, predictions, learning_rate, targets):\n",
        "\n",
        "  new_weights = weights\n",
        "\n",
        "  # para cada elemento de predictions fazer a correção dos pesos\n",
        "\n",
        "  for i in range(len(predictions)):\n",
        "\n",
        "    # a variação dos pesos vai ser igual a multiplicação do learning_rate pelas derivadas das funções e a derivada do erro em relação a saída\n",
        "    # para camadas intermediárias a saída ta conectada a todos os neurónios da proxima camada, então vai ter um somatório em relação ao erro\n",
        "\n",
        "    # vai armazenar o erro de cara unidade computacional de cada camada\n",
        "    parcial_loss = [[np.zeros(1) for k in range(hidden_layer_size[j])] for j in range(hidden_size)]\n",
        "\n",
        "    parcial_loss.append([np.zeros(1) for j in range(output_size)])\n",
        "\n",
        "    # para cada camada da rede, ir alterando os valores dos pesos\n",
        "\n",
        "    for j in range(hidden_size+1):\n",
        "\n",
        "      # alterando os pesos da camada de saída\n",
        "\n",
        "      if j == 0:\n",
        "\n",
        "        # k refere a camada anterior a camada de saída\n",
        "\n",
        "        for k in range(len(hidden_layer_size[-1])):\n",
        "\n",
        "          # l refere a camada de saída\n",
        "\n",
        "          for l in range(len(output_size)):\n",
        "\n",
        "            # derivada do erro pelo output gerado pela camada de saída, considerando o squared_error\n",
        "\n",
        "            derivate_error_output = predictions[i][-1][l][0]-targets[i][l]\n",
        "\n",
        "            # derivada da função pelo input (gerado pela camada anterior) da unidade computacional\n",
        "\n",
        "            if activation_functions[-1] == 'relu':\n",
        "\n",
        "              derivate_function_output = derivative_Relu(predictions[i][-1][l][0])\n",
        "\n",
        "            elif activation_functions[-1] == 'sigmoid':\n",
        "\n",
        "              derivate_function_output = derivative_Sigmoid(predictions[i][-1][l][0])\n",
        "\n",
        "            elif activation_functions[-1] == 'linear':\n",
        "\n",
        "              derivate_function_output = derivative_Linear(predictions[i][-1][l][0])\n",
        "\n",
        "            # derivada do input (gerado pela camada anterior) pelo peso analisado em questão\n",
        "\n",
        "            derivate_input_weight = predictions[i][-2][k][0]*new_weights[-1][k][l][0]\n",
        "\n",
        "            # atualizando o peso\n",
        "\n",
        "            new_weights[-1][k][l][0] = new_weights[-1][k][l][0] - learning_rate*derivate_error_output*derivate_function_output*derivate_input_weight\n",
        "\n",
        "            #armazenar o valor para calcular a derivada da camada anterior\n",
        "\n",
        "            parcial_loss[-1][k] = derivate_error_output*derivate_function_output\n",
        "\n",
        "      # caso a camada seja uma camada intermediária\n",
        "\n",
        "      else:\n",
        "\n",
        "        # k refere a camada anterior a camada queremos mudar os pesos\n",
        "\n",
        "        for k in range(len(hidden_layer_size[-2 - j])):\n",
        "\n",
        "          # l refere a camada de saída\n",
        "\n",
        "          for l in range(len(hidden_layer_size[-1 - j])):\n",
        "\n",
        "            # derivada do erro pelo output gerado pela camada de saída, considerando o squared_error\n",
        "            # como para camadas intermediárias, o erro gerado por elas afeta todas as unidades computacionais da próxima camada\n",
        "            # vai ocorrer um somatório desse erro\n",
        "\n",
        "            derivate_error_output = 0\n",
        "\n",
        "            for m in range(len(hidden_layer_size[-1 - j])):\n",
        "\n",
        "              derivate_error_output += predictions[i][k][m][0]*weights[-1 -j][k][m][0]\n",
        "\n",
        "\n",
        "            # a partir daqui eu não alterei ainda, precisa de alterações\n",
        "\n",
        "            # derivada da função pelo input (gerado pela camada anterior) da unidade computacional\n",
        "\n",
        "            if activation_functions[-1] == 'relu':\n",
        "\n",
        "              derivate_function_output = derivative_Relu(predictions[i][-1][l][0])\n",
        "\n",
        "            elif activation_functions[-1] == 'sigmoid':\n",
        "\n",
        "              derivate_function_output = derivative_Sigmoid(predictions[i][-1][l][0])\n",
        "\n",
        "            elif activation_functions[-1] == 'linear':\n",
        "\n",
        "              derivate_function_output = derivative_Linear(predictions[i][-1][l][0])\n",
        "\n",
        "            # derivada do input (gerado pela camada anterior) pelo peso analisado em questão\n",
        "\n",
        "            derivate_input_weight = predictions[i][-2][k][0]*new_weights[-1][k][l][0]\n",
        "\n",
        "            # atualizando o peso\n",
        "\n",
        "            new_weights[-1][k][l][0] = new_weights[-1][k][l][0] - learning_rate*derivate_error_output*derivate_function_output*derivate_input_weight\n",
        "\n",
        "\n",
        "\n",
        "  return new_weights"
      ],
      "metadata": {
        "id": "iyO_oT-D34ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# o treinamento fará no máximo MAX_EPOCH iterações\n",
        "# ele irá parar quando a acurácia dos testes diminuir em 5 épocas seguidas\n",
        "# após o processo de treinamento será retornado os pesos ótimos para o problema encontrado\n",
        "\n",
        "def treinamento(input_size, hidden_size, output_size, hidden_layer_size, activation_functions, MAX_EPOCH, learning_rate, batch_size_train, batch_size_test, weights, data_train, data_test, data_train_label, data_test_label):\n",
        "\n",
        "  for epoch in range(MAX_EPOCH):\n",
        "\n",
        "    # atualizando os mini-batchs de treino e de teste\n",
        "\n",
        "    batch_index = rng.choice(len(data_train), size = batch_size_train, replace = False)\n",
        "\n",
        "    batch_train = data_train[batch_index]\n",
        "\n",
        "    batch_train_label = data_train_label[batch_index]\n",
        "\n",
        "    batch_index = rng.choice(len(data_test), size = batch_size_test, replace = False)\n",
        "\n",
        "    batch_test = data_test[batch_index]\n",
        "\n",
        "    batch_test_label = data_test_label[batch_index]\n",
        "\n",
        "    #resultado da fase foward\n",
        "\n",
        "    predictions = foward(input_size, hidden_size, output_size, hidden_layer_size, activation_functions, weights, batch_train)\n",
        "\n",
        "    # calculando o erro gerado\n",
        "\n",
        "    error = mean_square_loss(batch_train_label, predictions)\n",
        "\n",
        "    # atualizando os pesos, fase backward\n",
        "\n",
        "\n",
        "  return weights"
      ],
      "metadata": {
        "id": "_05HMXoItKrt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treinamento(input_size, hidden_size, output_size, hidden_layer_size, activation_functions, MAX_EPOCH, learning_rate, data, data, weights, data, data, data, data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNRs33jhk59a",
        "outputId": "8df30f0f-e101-42ae-e199-3c2156c244a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.71071042]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[[0.70233802]],\n",
              " \n",
              "        [[0.60370907]]]),\n",
              " array([[[0.54416906]]])]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}