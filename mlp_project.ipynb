{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WECeh57QhL9O"
      ],
      "authorship_tag": "ABX9TyPLCNZSuKC3yJy3ejyuaLaq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrunoMog/PROJETOS-IF687/blob/main/mlp_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WECeh57QhL9O"
      },
      "source": [
        "## IF867 - Introdução à Aprendizagem Profunda\n",
        "\n",
        "### 1ª atividade prática\n",
        "\n",
        "Discente(s): Bruno Antonio dos Santos Bezerra\n",
        "\n",
        "Período: 7°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRkbCshWhfya"
      },
      "source": [
        "### Instruções e Requisitos\n",
        "- Objetivo: Implementar e treinar um Multilayer Perceptron (MLP), inteiramente em [NumPy](https://numpy.org/doc/stable/) ou [Numba](https://numba.readthedocs.io/en/stable/index.html), sem o uso de bibliotecas de aprendizado profundo.\n",
        "- A atividade pode ser feita em dupla.\n",
        "\n",
        "### Tarefas\n",
        "\n",
        "__Implementação (50%):__\n",
        "\n",
        "- Construa um MLP com uma camada de entrada, pelo menos duas camadas ocultas e uma camada de saída.\n",
        "- Implemente pelo menos duas funções de ativação diferentes para as camadas ocultas; use Sigmoid e Linear para a camada de saída.\n",
        "- Implemente forward e backpropagation.\n",
        "- Implemente um otimizador de sua escolha, adequado ao problema abordado.\n",
        "- Implemente as funções de treinamento e avaliação.\n",
        "\n",
        "__Aplicação (30%):__\n",
        "\n",
        "  Teste se os seus modelos estão funcionando bem com as seguintes tarefas:\n",
        "  - Regressão\n",
        "  - Classificação binária\n",
        "\n",
        "__Experimentação (20%):__\n",
        "\n",
        "  Teste os seus modelos com variações na arquitetura, no pré-processamento, etc. Escolha pelo menos uma das seguintes opções:\n",
        "  - Variações na inicialização de pesos\n",
        "  - Variações na arquitetura\n",
        "  - Implementação de técnicas de regularização\n",
        "  - Visualização das ativações e gradientes\n",
        "\n",
        "***Bônus:*** Implemente o MLP utilizando uma biblioteca de machine learning (ex.: [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/?hl=pt-br), [tinygrad](https://docs.tinygrad.org/), [Jax](https://jax.readthedocs.io/en/latest/quickstart.html)) e teste-o em uma das aplicações e em um dos experimentos propostos. O bônus pode substituir um dos desafios de aplicação ou experimentos feitos em NumPy, ou simplesmente somar pontos para a pontuação geral.\n",
        "\n",
        "### Datasets recomendados:\n",
        "Aqui estão alguns datasets recomendados, mas fica a cargo do aluno escolher os datasets que utilizará na atividade, podendo escolher um dataset não listado abaixo.\n",
        "- Classificação\n",
        "\n",
        "  - [Iris](https://archive.ics.uci.edu/dataset/53/iris)\n",
        "  - [Breast Cancer Wisconsin (Diagnostic)](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)\n",
        "  - [CDC Diabetes Health Indicators](https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators)\n",
        "\n",
        "- Regressão\n",
        "\n",
        "  - [Air Quality](https://archive.ics.uci.edu/dataset/360/air+quality)\n",
        "  - [Student Performance](https://archive.ics.uci.edu/dataset/320/student+performance)\n",
        "  - [Wine Quality](https://archive.ics.uci.edu/dataset/186/wine+quality)\n",
        "\n",
        "### Requisitos para Entrega\n",
        "\n",
        "Um notebook Jupyter (de preferência, o link do colab) ou script Python contendo:\n",
        "\n",
        "- Código: Implementação completa da MLP.\n",
        "- Gráficos e Análises: Gráficos da curva de perda, ativações, gradientes e insights do treinamento, resultantes dos experimentos com parada antecipada e diferentes técnicas de regularização.\n",
        "- Relatório: Um breve relatório detalhando o impacto de várias configurações de hiperparâmetros(ex.: inicialização de pesos, número de camadas ocultas e neurônios) e métodos de regularização no desempenho do modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Início da implementação\n"
      ],
      "metadata": {
        "id": "DFYW04wjZtDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importanto biblioteca\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8liRcfYnTpKX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setando parâmetros da rede mlp\n",
        "\n",
        "input_size = 4\n",
        "hidden_size = 5\n",
        "output_size = 3\n",
        "\n",
        "#cada camada intermediária pode ter o mesmo número de unidades computacionais para fins de simplificação ou apenas terem valores diferentes, nessa implementação vamos adotar a segunda abordagem\n",
        "#para cada camada intermediária - hidden layer, terá um valor relacionado a quantidade de unidades computacionais\n",
        "\n",
        "hidden_layer_size = [5, 5, 5, 5, 5]\n",
        "\n",
        "# serão implementadas três funções de ativação para as camadas intermediárias a relu, sigmoid e linear pela qual será indicada, e para a camdada de saída apenas sigmoid ou linear\n",
        "# a primeira função será para camadas intermediárias e a segunda para a camada de saída\n",
        "\n",
        "activation_functions = ['sigmoid', 'linear']\n",
        "\n",
        "# o número máximo de épocas que o modelo será treinado\n",
        "# o objetivo é que não precise chegar ao número máximo de épocas, quando o modelo apresentar overfitting o treinamento já acabe\n",
        "\n",
        "MAX_EPOCH = 100000\n",
        "\n",
        "# taxa de aprendizado que será utilizada\n",
        "\n",
        "learning_rate = 0.03\n",
        "\n",
        "# tamanho dos mini-batchs que será utilizado dos dados de treinamento e teste\n",
        "\n",
        "batch_size_train = 100\n",
        "\n",
        "batch_size_test = 20"
      ],
      "metadata": {
        "id": "MpHczcWLTxNI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setando o gerador de números aleatórios para setar os pesos iniciais\n",
        "\n",
        "weigths_seed = 115\n",
        "rng = np.random.default_rng(weigths_seed)\n",
        "\n",
        "# como a rede mlp é totalmente conectada, o número de pesos gerados entre camadas segirá sempre o padrão layer[i]*layer[i+1]\n",
        "\n",
        "weights = []\n",
        "\n",
        "for i in range(hidden_size+1):\n",
        "\n",
        "  if i == 0:\n",
        "\n",
        "    weights.append(rng.random(size = (input_size, hidden_layer_size[i], 1)))\n",
        "\n",
        "  elif i < hidden_size:\n",
        "\n",
        "    weights.append(rng.random((hidden_layer_size[i-1], hidden_layer_size[i], 1)))\n",
        "\n",
        "  else:\n",
        "\n",
        "    weights.append(rng.random((hidden_layer_size[i-1], output_size, 1)))\n",
        "\n"
      ],
      "metadata": {
        "id": "xJc4i5gCWSGx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# primeiro index é para referir a qual camada, sempre haverá hidden_layers+1 camada de pesos\n",
        "# o segundo index para referenciar qual unidade computacional da camada\n",
        "# o terceiro index para referenciar o peso a qual unidade computacional da próxima camada\n",
        "# o quarto index para obter o valor\n",
        "print(weights[5][0][0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1hNLJNRkptI",
        "outputId": "7b481c4a-4793-44f9-c7e3-0d1ca39407b6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9024653423513748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funções para o treinamento da rede MLP\n",
        "\n",
        "#### fase foward e backward\n"
      ],
      "metadata": {
        "id": "dvqQTwlOr02t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# funções de ativação\n",
        "\n",
        "# função relu normal\n",
        "def function_Relu(x):\n",
        "\n",
        "  return max(0, x)\n",
        "\n",
        "# função sigmoid normal\n",
        "def function_Sigmoid(x):\n",
        "\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "# função linear entre 0 e 1\n",
        "def function_Linear(x):\n",
        "\n",
        "  return max(0, min(1, x))"
      ],
      "metadata": {
        "id": "_n-jDHSrSlOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# processo de foward da rede\n",
        "\n",
        "def foward(input_size, hidden_size, output_size, hidden_layer_size, activation_functions, weights, data):\n",
        "\n",
        "  # o resultado de cada valor multiplicado pelo seu peso será armazenado no valor parcial\n",
        "\n",
        "  resultado_parcial = [[[] for j in range(hidden_layer_size[i])] for i in range(hidden_size)]\n",
        "\n",
        "  resultado_parcial.append([[]for i in range(output_size)])\n",
        "\n",
        "  # calcular o resultado camada por camada\n",
        "\n",
        "  for i in range(hidden_size+1):\n",
        "\n",
        "    # no primeiro caso será a camada de dados\n",
        "\n",
        "    if i == 0:\n",
        "\n",
        "      input = data\n",
        "\n",
        "    # o caso em que estamos calculando o resuldado das camadas intermediárias\n",
        "\n",
        "    if(i < hidden_size):\n",
        "\n",
        "      for j in range(len(input)):\n",
        "\n",
        "        for k in range(hidden_layer_size[i]):\n",
        "\n",
        "          resultado_parcial[i][k].append(weights[i][j][k][0]*input[j])\n",
        "\n",
        "\n",
        "      aux = np.zeros(hidden_layer_size[i])\n",
        "\n",
        "      for j in range(hidden_layer_size[i]):\n",
        "\n",
        "        aux[j] = np.sum(resultado_parcial[i][j])\n",
        "\n",
        "      # após ter os resultados parciais da soma ponderada das entradas daquela camada, esse valor irá passar pela função de ativação\n",
        "\n",
        "      if activation_functions[0] == 'relu':\n",
        "\n",
        "        for j in range(len(aux)):\n",
        "\n",
        "          aux[j] = function_Relu(aux[j])\n",
        "\n",
        "      elif activation_functions[0] == 'sigmoid':\n",
        "\n",
        "        for j in range(len(aux)):\n",
        "\n",
        "          aux[j] = function_Sigmoid(aux[j])\n",
        "\n",
        "      elif activation_functions[0] == 'linear':\n",
        "\n",
        "        for j in range(len(aux)):\n",
        "\n",
        "          aux[j] = function_Linear(aux[j])\n",
        "\n",
        "\n",
        "    # para o caso contrário\n",
        "\n",
        "    else:\n",
        "\n",
        "      for j in range(len(input)):\n",
        "\n",
        "        for k in range(output_size):\n",
        "\n",
        "          resultado_parcial[i][k].append(weights[i][j][k][0]*input[j])\n",
        "\n",
        "      aux = np.zeros(output_size)\n",
        "\n",
        "      for j in range(output_size):\n",
        "\n",
        "        aux[j] = np.sum(resultado_parcial[i][j])\n",
        "\n",
        "      # após ter os resultados parciais da soma ponderada das entradas daquela camada, esse valor irá passar pela função de ativação\n",
        "\n",
        "      if activation_functions[0] == 'relu':\n",
        "\n",
        "        for j in range(len(aux)):\n",
        "\n",
        "          aux[j] = function_Relu(aux[j])\n",
        "\n",
        "      elif activation_functions[0] == 'sigmoid':\n",
        "\n",
        "        for j in range(len(aux)):\n",
        "\n",
        "          aux[j] = function_Sigmoid(aux[j])\n",
        "\n",
        "      elif activation_functions[0] == 'linear':\n",
        "\n",
        "        for j in range(len(aux)):\n",
        "\n",
        "          aux[j] = function_Linear(aux[j])\n",
        "\n",
        "    # após ter o resultado parcial da camada, o input da próxima iteração será o resultado dessa\n",
        "\n",
        "    input = aux\n",
        "\n",
        "  return input\n",
        ""
      ],
      "metadata": {
        "id": "OSa5J3HDGoT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# o treinamento fará no máximo MAX_EPOCH iterações\n",
        "# ele irá parar quando a acurácia dos testes diminuir em 5 épocas seguidas\n",
        "# após o processo de treinamento será retornado os pesos ótimos para o problema encontrado\n",
        "\n",
        "def treinamento(input_size, hidden_size, output_size, hidden_layer_size, activation_functions, MAX_EPOCH, learning_rate, batch_size_train, batch_size_test, weights, data_train, data_test, data_train_label, data_test_label):\n",
        "\n",
        "  for epoch in range(MAX_EPOCH):\n",
        "\n",
        "    # atualizando os batchs de treino e de teste\n",
        "\n",
        "    batch_index = rng.integers(len(data_train), size = batch_size_train)\n",
        "\n",
        "    batch_train = data_train[batch_index]\n",
        "\n",
        "    batch_train_label = data_train_label[batch_index]\n",
        "\n",
        "    batch_index = rng.integers(len(data_test), size = batch_size_test)\n",
        "\n",
        "    batch_test = data_test[batch_index]\n",
        "\n",
        "    batch_test_label = data_test_label[batch_index]\n",
        "\n",
        "    # atualizando os pesos de acordo com o erro no batch do treino\n",
        "    # a função de custo será a MSE\n",
        "\n",
        "    for i in range(batch_size_train):\n",
        "\n",
        "      #resultado do processo de foward\n",
        "\n",
        "      foward_result = foward(input_size, hidden_size, output_size, hidden_layer_size, activation_functions, weights, batch_train[i])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return weights"
      ],
      "metadata": {
        "id": "_05HMXoItKrt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}